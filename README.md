# pupper-learned-gait-transitions
We explore learning-based strategies to teach an open source quadruped to deploy different gaits when running at different velocities

See the code we used to for simulation/learning with pupper: https://github.com/jadenvc/puppersim
See our changes to bullet3: https://github.com/jadenvc/bullet3

Fall 2021 Project Documentation

Motivation:
Humans and animals change their gait for different tasks, from walking and running to changes in terrain. From a biological perspective, a variety of muscles and tendons are engaged when humans run for different purposes. This process is also present in quadrupeds, specifically dogs, that adapt their locomotion depending on their gait, such as walking, trotting, cantering, and galloping. We used locomotion in animals as motivation behind our project, trying to emulate and learn locomotion based on desired velocity and terrain. We wanted a learned policy that could gradually adapt quadruped locomotion in a continuous manner, as opposed to discrete defined gaits, such as walking and trotting. The paper “Fast and Efficient Locomotion via Learned Gait Transitions” by Yuxiang Yang, et. al 2021, was used as reference for the project, using a high-level and low-level controller to generate a gait. This will be covered in detail later on, but the high-level controller takes desired velocity as input, outputting a contact schedule, and the low-level controller uses this contact schedule to find desired foot position and force, allowing the robot to walk.

Approach:
Implementing the hierarchical controller designed in Yang, et. al was our baseline goal, from which we could layer on more capability (adding vision, terrain adaptability, etc.). We bifurcated this implementation task into two workstreams: implementing the high-level controller and the low-level controller. To implement the high-level controller, we changed the observation space and action space to base/desired velocity and a contact schedule, respectively. As a proof-of-concept for the low-level controller, we first implemented PMTG, and observed whether a high-level and low-level controller integration was successful. We then went on to implement the pupper heuristic controller.

Results:
Most of our work this quarter involved re-engineering the bullet3 physics engine to use a raibert heuristic based open-loop controller and to accept different observations other than joint angles. We successfully trained pupper to learn to walk at high and low step frequencies based on a desired velocity. We also taught pupper to walk at high and low speeds within the PMTG framework. Pupper learned a visibly different contact schedule for trotting at different speeds. We also attempted to develop a single controller for both high and low desired speeds. We set up a framework for this in bullet3 (and also were able to test learned policies on the real pupper), but haven’t developed a stable learning curriculum for this controller yet. 


Lessons Learned/Reflections: Working on Pupper was one of the best parts of our fall quarter. Reinforcement learning was a foreign topic to us at the start of fall quarter, but we walked out with hands-on experience implementing a policy on a quadruped! For the first half of the class, we built Pupper. This part of the class brought exciting challenges that allowed us to learn more and become better problem solvers. For instance, when we were working on the inverse kinematics lab, we experienced some bugs that pushed us to debug beyond the software. Because at the time our math for the joint angles was not wrong, however, the results that we were getting were wrong. We tried debugging the code, and after some tries, we realized the orientation of the leg, when we did the calculations for the joint angles, was actually the reason why the code was giving us the results we were getting. This was our initial experience with debugging not only software but also hardware aspects of the project. Throughout the process of trying to deploy our policy for our final project, we learned the non-trivial aspects of reinforcement learning. In the end, we intend to continue working on this project and try to implement some of our other ideas. For instance, it would be cool to implement vision in the RL loop, which would allow pupper to learn to walk differently on different terrain. Another idea we have is to have an actual dog with cameras and motion sensors as an expert so that we can teach robots to walk in the same ways as animals on challenging terrain.
